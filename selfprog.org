* nature.cpp: program to emulate evolution of digital life
** aim
 produce super intelligence through natural selection
** broader picture
Roboter made of stainless steel can survive longer than humans. Human biology makes it frail to changes in timescales of 10^6 years and above (e.g. rising temperatures).

Way to transcend intelligence: Put artificial intelligence into a robot which would be able to repair itself and duplicate.

for basic 4bit ALU:
http://www.tomshardware.co.uk/forum/308174-28-build-basic

** optimization
*** Software
- desire to survive must be stronger than desire to develop/progress, otherwise, the species might face extinction
- at early stages: resources are CPU-time/RAM

** energy source:
- sun light. nothing else is available on longer timescales

* File management
** basis files
selfprog/

** work directory
/tmp/cell

in main memory, virtual filesystem, to keep read/write interaction time fast

** training files in PATH_TRAINING
selfprog/training/
are copied to PATH_CELL/{input, export} before program starts
TODO: run through several examples, add up performance

** file flow
*** get one of last run's cells as parent cell
if debugging: fine starting point, PATH_PROG/cell
if not:       a random cell from /tmp/cell/reproduce/cell*

*** set up genepool
- for newly created cells

* Ideas
** reproduction scheme
We have to find a tradeoff between number of changes we make, and number of allowed genes at any given point in time.
In the one extreme, we could change a high number of bytes of the cell, get huge leaps in evolution, but risk non-reproductivity.
The other extreme sees the smallest possible changes, only 1 byte at a time, best possible survivability, but really slow evolution.
What is the sweet spot?

*** uniform random distribution for number of changes
    // determine number of bytes to get changed
    // unsigned int cycles = rand() % 10;

*** Poisson distribution for number of changes
    // better: poisson distributed:
    // this allows for more aggressive changes (count>1), while still keeping most power on small count values,
    // thus guaranteeing
    //    1. many easy changes (higher probability to succeed in compilation and reproduction)
    //    2. evasion of any minimum where more than one change is necessary to get out of

*** number of changes depending on..
 - length of program
 - performance of program

** genepool management
*** large genepool
         Got to make sure the program can run with very large number of programs in the genepool,
         e.g. with moving window (last N genes)
         or sending to database in background (and reading in most used genes)
         OR: not caring about full history, take any of the reproducing programs and start genepool from them
         this way, we can have several instances of nature.cpp run in parallel, each with a different starting program,
         and after N iterations, another instance is started instead

    if(iteration % 100 == 0){
     output multimap to file for later reference
     std::multimap<vuc, vuc>::iterator it = genepool.first;
     ...
    }
